{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Baseline.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFzogzH5Q4XY","executionInfo":{"status":"ok","timestamp":1657180461469,"user_tz":-120,"elapsed":2138,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}},"outputId":"8d90a36f-53f6-4d5d-ccdc-d0c3bb195f7b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /google_drive; to attempt to forcibly remount, call drive.mount(\"/google_drive\", force_remount=True).\n"]}],"source":["#Reading the database\n","import pandas as pd\n","#pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', -1)\n","\n","\n","from google.colab import drive\n","drive.mount(\"/google_drive\")\n","WORKSPACE_PATH = \"/google_drive/My Drive/MBD Capstone\"\n","\n","xls = pd.ExcelFile(WORKSPACE_PATH+ '/ie_input_data_no_character_sin_no.xlsx')\n","df = pd.read_excel(xls, 'Sheet1')"]},{"cell_type":"markdown","source":["### Baseline Model NB, bag of words"],"metadata":{"id":"ywg-TGDHqspY"}},{"cell_type":"code","source":["#Choosing the columns to work and rename the columns\n","df = df.set_index('unique_id')\n","df = df[['text_answers','show_up']]\n","df.columns = ['text','labels']"],"metadata":{"id":"hVmV6ltkRL9-","executionInfo":{"status":"ok","timestamp":1657180461469,"user_tz":-120,"elapsed":7,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# splitting data in train and test\n","from sklearn.model_selection import train_test_split\n","X_train, X_test = train_test_split(df, stratify=df['labels'])\n","\n","\n","# Resampling the dataset to balance it\n","from imblearn.over_sampling import RandomOverSampler\n","ros = RandomOverSampler(random_state=0)\n","X_trainR, y_trainR = ros.fit_resample(X_train[['text']], X_train['labels'])\n","X_testR, y_testR = ros.fit_resample(X_test[['text']], X_test['labels'])"],"metadata":{"id":"F_mDMmFMlcua","executionInfo":{"status":"ok","timestamp":1657180461469,"user_tz":-120,"elapsed":6,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["#Create a formula that eliminates the punctuation and put words in lower case\n","import string\n","\n","def pre_process_text(data,col='text'):\n","\n","  data[col] = data[col].astype('str')\n","  data[col] = data[col].str.replace('\\|\\|',' ')\n","  data[col] = [s.translate(str.maketrans('', '', string.punctuation)) for s in data[col]]\n","  data[col] = data[col].str.lower()\n","  return data"],"metadata":{"id":"Cpiz6tS3Rwzi","executionInfo":{"status":"ok","timestamp":1657180461470,"user_tz":-120,"elapsed":6,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["#Create a formula that deletes everything that is not a word, deleting the stop words, and do stemming (root word)\n","import nltk\n","from nltk.stem import *\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","nltk.download(\"stopwords\")\n","\n","import re\n","\n","def process_text(raw_text):\n","\n","    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_text) \n","    words = letters_only.lower().split()\n","    \n","    stops = set(stopwords.words(\"spanish\")) \n","    stops.remove('no')\n","    not_stop_words = [w for w in words if not w in stops]\n","    \n","    stemmer = SnowballStemmer('spanish')\n","    stemmed = [stemmer.stem(word) for word in not_stop_words]\n","    \n","    return( \" \".join( stemmed ))  \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uu05FMXkVrI6","executionInfo":{"status":"ok","timestamp":1657180461472,"user_tz":-120,"elapsed":8,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}},"outputId":"a1e26dff-7e22-4fa7-ea59-776ac9e6fa42"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["#Apply formulas to the train and test set\n","X_trainR = pre_process_text(X_trainR,'text')\n","X_trainR['text'] = X_trainR['text'].apply(lambda x: process_text(x))\n","\n","X_testR = pre_process_text(X_testR,'text')\n","X_testR['text'] = X_testR['text'].apply(lambda x: process_text(x))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"merD3PkYW3gQ","executionInfo":{"status":"ok","timestamp":1657180462015,"user_tz":-120,"elapsed":548,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}},"outputId":"78cd8944-fbc8-45ea-a0cb-1b56bbdc65b6"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n","  import sys\n"]}]},{"cell_type":"code","source":["# Crate a dataframe of bag of words with tf_idf weight\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n","count_vect = CountVectorizer(analyzer = \"word\")\n","train_features = count_vect.fit_transform(X_trainR['text'])\n","test_features = count_vect.transform(X_testR['text'])\n","\n","tfidf = TfidfTransformer(norm=\"l2\")\n","train_text_tfidf_features = tfidf.fit_transform(train_features)\n","test_text_tfidf_features = tfidf.fit_transform(test_features)   "],"metadata":{"id":"5ybWENnkTsxb","executionInfo":{"status":"ok","timestamp":1657180462016,"user_tz":-120,"elapsed":4,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# apply Naive Baises to the Data\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","\n","clf = MultinomialNB()\n","\n","clf.fit(train_text_tfidf_features,y_trainR)\n","\n","y_train_pred = clf.predict(train_text_tfidf_features)\n","y_test_pred = clf.predict(test_text_tfidf_features)\n","\n","acc_train = accuracy_score(y_trainR, y_train_pred)\n","acc_test = accuracy_score(y_testR, y_test_pred)\n","\n","print(f' train accuracy: {acc_train:.3}, test accuracy: {acc_test:.3}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LvxFz6Z-UHnj","executionInfo":{"status":"ok","timestamp":1657180462016,"user_tz":-120,"elapsed":3,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}},"outputId":"98367cf8-c7d5-4920-a46b-678a940f753c"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":[" train accuracy: 0.694, test accuracy: 0.383\n"]}]},{"cell_type":"markdown","source":["### Baseline model Logit and Random Forest with numeric values"],"metadata":{"id":"1RcmOf1Jq0Dt"}},{"cell_type":"code","source":["#Read the data\n","xls = pd.ExcelFile(WORKSPACE_PATH+ '/JLB/numeric_vals.xlsx')\n","df_num = pd.read_excel(xls, 'Sheet1')"],"metadata":{"id":"6r0STPLvq4cG","executionInfo":{"status":"ok","timestamp":1657180462687,"user_tz":-120,"elapsed":673,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# list(df.index)"],"metadata":{"id":"QqCBanD2o_JA","executionInfo":{"status":"ok","timestamp":1657180462688,"user_tz":-120,"elapsed":5,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["df_num = df_num.merge(df.reset_index()[['unique_id']])"],"metadata":{"id":"LpMtxCKdpYRk","executionInfo":{"status":"ok","timestamp":1657180462688,"user_tz":-120,"elapsed":4,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["#columns to use that are the ones that are numeric\n","cols = list(df_num.columns[df_num.columns.str.contains('\\|')])\n","df_num = df_num[cols+['show_up']]\n"],"metadata":{"id":"JBo5Up-Jrl82","executionInfo":{"status":"ok","timestamp":1657180462688,"user_tz":-120,"elapsed":4,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["\n","# splitting data in train and test\n","from sklearn.model_selection import train_test_split\n","X_train, X_test = train_test_split(df_num, stratify=df_num['show_up'])\n","\n","# Resampling the dataset to balance it\n","from imblearn.over_sampling import RandomOverSampler\n","ros = RandomOverSampler(random_state=0)\n","X_trainR, y_trainR = ros.fit_resample(X_train[cols], X_train['show_up'])\n","X_testR, y_testR = ros.fit_resample(X_test[cols], X_test['show_up'])"],"metadata":{"id":"xc7IuyC0rlH4","executionInfo":{"status":"ok","timestamp":1657180462689,"user_tz":-120,"elapsed":4,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["#scaling data to use with logit\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","scaled = scaler.fit_transform(X_trainR)\n","X_trainR = pd.DataFrame(scaled, columns= X_trainR.columns)\n","scaled_t = scaler.transform(X_testR)\n","X_testR = pd.DataFrame(scaled_t, columns= X_testR.columns)"],"metadata":{"id":"IfDIBrkutl56","executionInfo":{"status":"ok","timestamp":1657180462689,"user_tz":-120,"elapsed":4,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["#apply logit\n","from sklearn.linear_model import LogisticRegression\n","\n","clf = LogisticRegression(random_state =41)\n","\n","clf.fit(X_trainR,y_trainR)\n","\n","y_train_pred = clf.predict(X_trainR)\n","y_test_pred = clf.predict(X_testR)\n","\n","acc_train = accuracy_score(y_trainR, y_train_pred)\n","acc_test = accuracy_score(y_testR, y_test_pred)\n","\n","print(f' train accuracy: {acc_train:.3}, test accuracy: {acc_test:.3}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vOrRgPKZrRZx","executionInfo":{"status":"ok","timestamp":1657180462883,"user_tz":-120,"elapsed":198,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}},"outputId":"3f283c93-64fb-429b-db54-fc62069f571a"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":[" train accuracy: 0.562, test accuracy: 0.419\n"]}]},{"cell_type":"code","source":["#apply random forest\n","from sklearn.ensemble import RandomForestClassifier\n","\n","clf = RandomForestClassifier(random_state =41)\n","\n","clf.fit(X_trainR,y_trainR)\n","\n","y_train_pred = clf.predict(X_trainR)\n","y_test_pred = clf.predict(X_testR)\n","\n","acc_train = accuracy_score(y_trainR, y_train_pred)\n","acc_test = accuracy_score(y_testR, y_test_pred)\n","\n","print(f' train accuracy: {acc_train:.3}, test accuracy: {acc_test:.3}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VRn_8vd0rP2j","executionInfo":{"status":"ok","timestamp":1657180463440,"user_tz":-120,"elapsed":559,"user":{"displayName":"Juan Luis Betancur","userId":"13282590302337783985"}},"outputId":"d35ffe9f-7dff-4944-9912-070126028a73"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":[" train accuracy: 0.999, test accuracy: 0.472\n"]}]}]}